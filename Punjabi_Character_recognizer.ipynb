{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prabhatpathak77/Punjabi-character-set-recognition-SLM-/blob/main/Punjabi_Character_recognizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "oiXFskCQj5gJ",
        "outputId": "2ba3515d-6f88-4354-a64b-3ab57fd71149"
      },
      "source": [
        "\"\"\"\n",
        "Train a Punjabi character recognizer that works on BOTH\n",
        "printed (synthetic) and handwritten characters.\n",
        "\n",
        "What you get in ONE file:\n",
        "- Character set ( vowels + consonants + extended letters)\n",
        "- Synthetic dataset generator (printed) using TrueType fonts\n",
        "- Data directory setup & train/val split\n",
        "- PyTorch Dataset/DataLoader with strong augmentations\n",
        "- Compact CNN (ResNet18) fine‑tuned for 48 classes\n",
        "- Mixed‑precision training, early stopping, checkpointing\n",
        "- Evaluation (accuracy + confusion matrix)\n",
        "- Simple inference helper\n",
        "\n",
        "USAGE (typical):\n",
        "1) Put a few Punjabi fonts (TTF) inside:  assets/fonts/\n",
        "   Examples: Raavi.ttf, AnmolUni.ttf, GurbaniAkhar.ttf (any fonts that support Punjabi)\n",
        "\n",
        "2) (Optional but recommended) Add your OWN handwritten samples here:\n",
        "   data/raw_handwritten/<label>/*.png  (label is the literal character, e.g. \"ਕ\")\n",
        "   You can have multiple images per character. They’ll be merged with synthetic data.\n",
        "\n",
        "3) Generate synthetic printed images + split data + train:\n",
        "   Set `generate = True` and `train = True` below, then run the cell.\n",
        "\n",
        "4) Evaluate and predict a single image:\n",
        "   Set `eval = True` or `predict = \"path/to/image.png\"` below, then run the cell.\n",
        "\n",
        "Folders that will be created:\n",
        "- data/synth/{train,val}/<label>/xxx.png      (synthetic printed)\n",
        "- data/handwritten/{train,val}/<label>/xxx.png (your raw handwritten split)\n",
        "- data/final/{train,val}/<label>/xxx.png      (merged for training)\n",
        "- checkpoints/best_model.pt                    (best weights)\n",
        "\n",
        "Tested with PyTorch 2.x, torchvision, Pillow, scikit-image, numpy.\n",
        "\"\"\"\n",
        "\n",
        "# -----------------------------\n",
        "# SET YOUR ARGS HERE\n",
        "# -----------------------------\n",
        "generate = True # Set to True to generate synthetic data and split handwritten data\n",
        "train = False   # Set to True to train the model\n",
        "eval = False    # Set to True to evaluate the model on the validation set\n",
        "predict = \"/WhatsApp Image 2025-08-22 at 10.35.44 (1).jpeg\"  # Set to a path like \"data/final/val/ਕ/hand_00001.png\" to predict a single image\n",
        "\n",
        "# Data/Model paths\n",
        "from pathlib import Path\n",
        "fonts_dir = Path(\"assets/fonts\")              # Folder with .ttf Punjabi fonts\n",
        "raw_hand_dir = Path(\"data/raw_handwritten\")   # Your handwritten source images by label\n",
        "synth_out_dir = Path(\"data/synth\")            # Where to place synthetic dataset\n",
        "hand_out_dir = Path(\"data/handwritten\")       # Where to split handwritten\n",
        "final_out_dir = Path(\"data/final\")            # Merged dataset root\n",
        "checkpoint_path = Path(\"checkpoints/best_model.pt\") # Path to save/load the best model\n",
        "\n",
        "# Training parameters\n",
        "images_per_char = 300\n",
        "epochs = 20\n",
        "batch_size = 128\n",
        "lr = 1e-3\n",
        "img_size = 96\n",
        "patience = 5\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# IMPORTS\n",
        "# -----------------------------\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw, ImageFont, ImageOps, ImageFilter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 1) CHARACTER SET\n",
        "# -----------------------------\n",
        "# You can edit this list to match exactly what you want to classify.\n",
        "CHARSET = [\n",
        "    # Vowels\n",
        "    \"ਅ\",\"ਆ\",\"ਇ\",\"ਈ\",\"ਉ\",\"ਊ\",\"ਏ\",\"ਐ\",\"ਓ\",\"ਔ\",\n",
        "    # Consonants\n",
        "    \"ਕ\",\"ਖ\",\"ਗ\",\"ਘ\",\"ਙ\",\n",
        "    \"ਚ\",\"ਛ\",\"ਜ\",\"ਝ\",\"ਞ\",\n",
        "    \"ਟ\",\"ਠ\",\"ਡ\",\"ਢ\",\"ਣ\",\n",
        "    \"ਤ\",\"ਥ\",\"ਦ\",\"ਧ\",\"ਨ\",\n",
        "    \"ਪ\",\"ਫ\",\"ਬ\",\"ਭ\",\"ਮ\",\n",
        "    # Sibilants & h\n",
        "    \"ਯ\",\"ਰ\",\"ਲ\",\"ਵ\",\n",
        "    \"ਸ਼\",\"ਸ\",\"ਹ\",\n",
        "    # Nukta/extended\n",
        "    \"ਖ਼\",\"ਗ਼\",\"ਜ਼\",\"ਫ਼\",\"ਲ਼\"\n",
        "]\n",
        "\n",
        "NUM_CLASSES = len(CHARSET)\n",
        "LABEL_TO_IDX = {ch: i for i, ch in enumerate(CHARSET)}\n",
        "IDX_TO_LABEL = {i: ch for ch, i in LABEL_TO_IDX.items()}\n",
        "\n",
        "# -----------------------------\n",
        "# 2) UTILS\n",
        "# -----------------------------\n",
        "\n",
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def ensure_dir(p: Path):\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 3) SYNTHETIC DATA GENERATOR (PRINTED)\n",
        "# -----------------------------\n",
        "\n",
        "def _rand_affine(img: Image.Image):\n",
        "    \"\"\"Random affine transform with slight rotation, shear, and translate.\"\"\"\n",
        "    # PIL Image.affine takes a 6-tuple (a, b, c, d, e, f) for\n",
        "    # x_new = ax + by + c\n",
        "    # y_new = dx + ey + f\n",
        "    # We want slight rotation, shear, and translation.\n",
        "    # This corresponds to:\n",
        "    # [ cos(theta) -sin(theta) tx ]\n",
        "    # [ sin(theta)  cos(theta) ty ]\n",
        "    # [ 0           0          1  ]\n",
        "    # [ 0           0          1  ]\n",
        "    # Then potentially add shear.\n",
        "    angle = random.uniform(-5, 5)  # degrees\n",
        "    shear_x = random.uniform(-3, 3) # degrees\n",
        "    tx = random.uniform(-0.02, 0.02) * img.size[0] # pixels\n",
        "    ty = random.uniform(-0.02, 0.02) * img.size[1] # pixels\n",
        "\n",
        "    # Rotation matrix\n",
        "    rad = math.radians(angle)\n",
        "    cos = math.cos(rad)\n",
        "    sin = math.sin(rad)\n",
        "\n",
        "    # Shear matrix (applied after rotation)\n",
        "    shear_x_rad = math.radians(shear_x)\n",
        "    shear_matrix = (1, math.tan(shear_x_rad), 0, 0, 1, 0)\n",
        "\n",
        "    # Combined (approximate) transform\n",
        "    # x_new = cos*x - sin*y + tx + tan(shear_x)*(sin*x + cos*y + ty)\n",
        "    # y_new = sin*x + cos*y + ty\n",
        "    # This is not exactly a clean affine matrix, but PIL handles it.\n",
        "    # A simpler approach is to apply transforms sequentially, but that's slower.\n",
        "    # Let's just use a simple affine matrix construction for small angles/shears.\n",
        "    # x_new = cos*x + (tan(shear_x)*cos - sin)*y + tx + tan(shear_x)*ty\n",
        "    # y_new = sin*x + cos*y + ty\n",
        "\n",
        "    # A common way to approximate the affine matrix for small rotations and shears\n",
        "    # is to combine them:\n",
        "    # cos(angle)*cos(shear) - sin(angle)*sin(shear)   -cos(angle)*sin(shear) - sin(angle)*cos(shear)\n",
        "    # sin(angle)*cos(shear) + cos(angle)*sin(shear)   -sin(angle)*sin(shear) + cos(angle)*cos(shear)\n",
        "    #\n",
        "    # For small angles: cos approx 1, sin approx angle.\n",
        "    # shear_x in PIL affine seems to be applied to x coordinates based on y.\n",
        "    # x_new = ax + by + c\n",
        "    # y_new = dx + ey + f\n",
        "    # For rotation by angle and shear_x:\n",
        "    # a = cos(angle)\n",
        "    # b = -sin(angle) + shear_x * cos(angle)\n",
        "    # d = sin(angle)\n",
        "    # e = cos(angle) + shear_x * sin(angle)\n",
        "\n",
        "    # Let's keep it simpler and closer to Pillow's docs for affine:\n",
        "    # (a, b, c, d, e, f) where (x', y') = (ax + by + c, dx + ey + f)\n",
        "    # Identity is (1, 0, 0, 0, 1, 0)\n",
        "    # Rotation by angle: (cos(a), -sin(a), 0, sin(a), cos(a), 0)\n",
        "    # Shear x by angle: (1, tan(a), 0, 0, 1, 0)\n",
        "    # Translation: (1, 0, tx, 0, 1, ty)\n",
        "\n",
        "    # We can combine these matrices (rotation * shear * translate), but PIL's affine\n",
        "    # is usually applied with respect to the center of the image implicitly or explicitly.\n",
        "    # A simpler way that matches Pillow's expected tuple:\n",
        "    # Use get_affine_matrix from torchvision or calculate manually\n",
        "    # For small transforms around center (cx, cy):\n",
        "    # x_new = a*(x-cx) + b*(y-cy) + cx + tx\n",
        "    # y_new = d*(x-cx) + e*(y-cy) + cy + ty\n",
        "    # x_new = ax + by + (c - acx - bcy + cx + tx)\n",
        "    # y_new = dx + ey + (f - dcx - ecy + cy + ty)\n",
        "\n",
        "    # A practical way to apply rotation, translation, and shear with PIL's affine is\n",
        "    # to build the matrix components:\n",
        "    # Rotation + Shear (combined matrix):\n",
        "    # R = [cos, -sin], [sin, cos]\n",
        "    # S = [1, tan(shear)], [0, 1]\n",
        "    # RS = [cos, tan(shear)*cos - sin], [sin, tan(shear)*sin + cos]\n",
        "    #\n",
        "    # Let's use a simplified approach for slight transforms:\n",
        "    # Translate to origin, rotate, shear, translate back, then add final translation\n",
        "    # This is too complex for the PIL affine tuple.\n",
        "\n",
        "    # Back to the simple affine tuple structure:\n",
        "    # (a, b, c, d, e, f)\n",
        "    a = 1.0 + random.uniform(-0.03, 0.03)\n",
        "    b = random.uniform(-0.05, 0.05)\n",
        "    c = random.uniform(-img.size[0] * 0.04, img.size[0] * 0.04)\n",
        "    d = random.uniform(-0.05, 0.05)\n",
        "    e = 1.0 + random.uniform(-0.03, 0.03)\n",
        "    f = random.uniform(-img.size[1] * 0.04, img.size[1] * 0.04)\n",
        "\n",
        "    return img.transform(\n",
        "        img.size,\n",
        "        Image.AFFINE,\n",
        "        (a, b, c, d, e, f),\n",
        "        fillcolor=255, # Fill with white\n",
        "        resample=random.choice([Image.BICUBIC, Image.BILINEAR])\n",
        "    )\n",
        "\n",
        "\n",
        "def generate_synthetic_dataset(\n",
        "    fonts_dir: Path,\n",
        "    out_root: Path,\n",
        "    images_per_char: int = 300,\n",
        "    canvas_size: int = 96,\n",
        "):\n",
        "    \"\"\"Render each class (character) using multiple fonts, sizes, and jitters.\"\"\"\n",
        "    ensure_dir(out_root / \"train\")\n",
        "    ensure_dir(out_root / \"val\")\n",
        "\n",
        "    # Recursively search for TTF files\n",
        "    font_paths = [p for p in fonts_dir.glob(\"**/*.ttf\")]\n",
        "    if not font_paths:\n",
        "        print(f\"No TTF fonts found in {fonts_dir} or its subdirectories. Please add Gurmukhi fonts (*.ttf). Skipping synthetic data generation.\")\n",
        "        return\n",
        "\n",
        "    for ch in CHARSET:\n",
        "        for split in [\"train\", \"val\"]:\n",
        "            ensure_dir(out_root / split / ch)\n",
        "\n",
        "        for i in range(images_per_char):\n",
        "            font_path = random.choice(font_paths)\n",
        "            # random font size tuned for 96x96 canvas\n",
        "            fsize = random.randint(int(canvas_size * 0.55), int(canvas_size * 0.85))\n",
        "            font = ImageFont.truetype(str(font_path), fsize)\n",
        "\n",
        "            img = Image.new(\"L\", (canvas_size, canvas_size), color=255)\n",
        "            draw = ImageDraw.Draw(img)\n",
        "            bbox = draw.textbbox((0, 0), ch, font=font)\n",
        "            tw, th = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
        "            x = (canvas_size - tw) // 2 - bbox[0]\n",
        "            # in Gurmukhi, headline (siari) can get clipped; nudge down a bit\n",
        "            y = (canvas_size - th) // 2 - bbox[1] + random.randint(-2, 4)\n",
        "            draw.text((x, y), ch, font=font, fill=0)\n",
        "\n",
        "            # Augmentations specific to printed text\n",
        "            if random.random() < 0.8:\n",
        "                img = _rand_affine(img)\n",
        "            if random.random() < 0.25:\n",
        "                img = img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.2, 0.8)))\n",
        "            if random.random() < 0.25:\n",
        "                # slight contrast/inversion trick for robustness\n",
        "                img = ImageOps.autocontrast(img)\n",
        "\n",
        "            # Normalize canvas back to fixed size after affine\n",
        "            img = ImageOps.contain(img, (canvas_size, canvas_size), Image.BICUBIC)\n",
        "            final = Image.new(\"L\", (canvas_size, canvas_size), 255)\n",
        "            ox = (canvas_size - img.size[0]) // 2\n",
        "            oy = (canvas_size - img.size[1]) // 2\n",
        "            final.paste(img, (ox, oy))\n",
        "\n",
        "            split = \"val\" if i % 10 == 0 else \"train\"  # ~10% val\n",
        "            fname = f\"{ch}_{i:05d}.png\"\n",
        "            final.save(out_root / split / ch / fname)\n",
        "\n",
        "    print(f\"✅ Synthetic dataset generated at: {out_root}\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 4) MERGE HANDWRITTEN + SYNTHETIC & SPLIT\n",
        "# -----------------------------\n",
        "\n",
        "def split_handwritten(raw_src: Path, out_root: Path, val_ratio: float = 0.1):\n",
        "    \"\"\"Split your own handwritten images into train/val under the same label folders.\n",
        "    Assumes raw_src/<label>/*.png or *.jpg\n",
        "    \"\"\"\n",
        "    for ch in CHARSET:\n",
        "        src = raw_src / ch\n",
        "        if not src.exists():\n",
        "            continue\n",
        "        imgs = [p for p in src.glob(\"*.*\") if p.suffix.lower() in {\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\"}]\n",
        "        if not imgs:\n",
        "            continue\n",
        "        random.shuffle(imgs)\n",
        "        n_val = max(1, int(len(imgs) * val_ratio))\n",
        "        val_imgs = imgs[:n_val]\n",
        "        train_imgs = imgs[n_val:]\n",
        "        for split, subset in [(\"train\", train_imgs), (\"val\", val_imgs)]:\n",
        "            dst = out_root / split / ch\n",
        "            ensure_dir(dst)\n",
        "            for i, p in enumerate(subset):\n",
        "                shutil.copy2(p, dst / f\"hand_{i:05d}{p.suffix.lower()}\")\n",
        "    print(f\"📦 Handwritten split into: {out_root}\")\n",
        "\n",
        "\n",
        "def merge_datasets(synth_root: Path, hand_root: Path, final_root: Path):\n",
        "    # Copy both into final/<split>/<label>/\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        for ch in CHARSET:\n",
        "            dst = final_root / split / ch\n",
        "            ensure_dir(dst)\n",
        "            for src_root in [synth_root, hand_root]:\n",
        "                src = src_root / split / ch\n",
        "                if src.exists():\n",
        "                    for p in src.glob(\"*.*\"):\n",
        "                        shutil.copy2(p, dst / p.name)\n",
        "    print(f\"🔗 Merged datasets into: {final_root}\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 5) DATASET & TRANSFORMS\n",
        "# -----------------------------\n",
        "\n",
        "def build_dataloaders(final_root: Path, batch_size: int = 128, img_size: int = 96):\n",
        "    # Handwritten often has stroke width variance and background; keep strong augs on train\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.Grayscale(),\n",
        "        transforms.RandomApply([transforms.GaussianBlur(3)], p=0.15),\n",
        "        transforms.RandomAffine(degrees=8, translate=(0.05, 0.05), shear=6),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,)),\n",
        "    ])\n",
        "\n",
        "    val_tf = transforms.Compose([\n",
        "        transforms.Grayscale(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,)),\n",
        "    ])\n",
        "\n",
        "    train_ds = datasets.ImageFolder(str(final_root / \"train\"), transform=train_tf)\n",
        "    val_ds = datasets.ImageFolder(str(final_root / \"val\"), transform=val_tf)\n",
        "\n",
        "    # Sanity check on class order -> save mapping\n",
        "    class_to_idx = train_ds.class_to_idx\n",
        "    with open(\"class_mapping.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(class_to_idx, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "    return train_loader, val_loader, class_to_idx\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 6) MODEL\n",
        "# -----------------------------\n",
        "\n",
        "def build_model(num_classes: int = NUM_CLASSES):\n",
        "    model = models.resnet18(weights=None)  # keep it light; from scratch is fine here\n",
        "    # Change first conv to accept 1 channel\n",
        "    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 7) TRAINING LOOP\n",
        "# -----------------------------\n",
        "\n",
        "def train(\n",
        "    final_root: Path,\n",
        "    epochs: int = 20,\n",
        "    batch_size: int = 128,\n",
        "    lr: float = 1e-3,\n",
        "    img_size: int = 96,\n",
        "    patience: int = 5,\n",
        "    device: str | None = None,\n",
        "    checkpoint_path: Path = Path(\"checkpoints/best_model.pt\") # Added checkpoint_path\n",
        "):\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    train_loader, val_loader, class_to_idx = build_dataloaders(final_root, batch_size, img_size)\n",
        "\n",
        "    model = build_model(len(class_to_idx)).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(device == \"cuda\"))\n",
        "\n",
        "    best_val = 0.0\n",
        "    epochs_no_improve = 0\n",
        "    ensure_dir(Path(\"checkpoints\"))\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, preds = outputs.max(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = running_loss / total\n",
        "        train_acc = correct / total\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        v_correct, v_total = 0, 0\n",
        "        v_loss_total = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                v_loss_total += loss.item() * images.size(0)\n",
        "                _, preds = outputs.max(1)\n",
        "                v_correct += (preds == labels).sum().item()\n",
        "                v_total += labels.size(0)\n",
        "        val_loss = v_loss_total / v_total\n",
        "        val_acc = v_correct / v_total\n",
        "\n",
        "        print(f\"Epoch {epoch:02d} | train loss {train_loss:.4f} acc {train_acc:.4f} | val loss {val_loss:.4f} acc {val_acc:.4f}\")\n",
        "\n",
        "        # early stopping on val_acc\n",
        "        if val_acc > best_val:\n",
        "            best_val = val_acc\n",
        "            epochs_no_improve = 0\n",
        "            torch.save({\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"class_to_idx\": class_to_idx,\n",
        "            }, checkpoint_path) # Use checkpoint_path\n",
        "            print(f\"  ↳ ✅ Saved new best (val acc={val_acc:.4f})\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(\"  ↳ Early stopping\")\n",
        "                break\n",
        "\n",
        "    print(f\"Best val acc: {best_val:.4f}\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 8) EVALUATION\n",
        "# -----------------------------\n",
        "\n",
        "def evaluate(final_root: Path, checkpoint_path: Path = Path(\"checkpoints/best_model.pt\")):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    _, val_loader, _ = build_dataloaders(final_root, batch_size=256)\n",
        "\n",
        "    if not checkpoint_path.exists():\n",
        "        print(f\"Error: Checkpoint not found at {checkpoint_path}. Please train the model first.\")\n",
        "        return\n",
        "\n",
        "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
        "    class_to_idx = ckpt[\"class_to_idx\"]\n",
        "    idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
        "\n",
        "    model = build_model(len(class_to_idx))\n",
        "    model.load_state_dict(ckpt[\"model_state\"])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = outputs.max(1)\n",
        "            y_true.extend(labels.cpu().numpy().tolist())\n",
        "            y_pred.extend(preds.cpu().numpy().tolist())\n",
        "\n",
        "    acc = (np.array(y_true) == np.array(y_pred)).mean()\n",
        "    print(f\"Validation accuracy: {acc:.4f}\")\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(idx_to_class))))\n",
        "    print(\"Confusion matrix (rows=true, cols=pred):\")\n",
        "    np.set_printoptions(linewidth=160)\n",
        "    print(cm)\n",
        "\n",
        "    target_names = [idx_to_class[i] for i in range(len(idx_to_class))]\n",
        "    print(classification_report(y_true, y_pred, target_names=target_names))\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 9) INFERENCE\n",
        "# -----------------------------\n",
        "def predict_image(img_path: Path, checkpoint_path: Path = Path(\"checkpoints/best_model.pt\")):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    if not checkpoint_path.exists():\n",
        "        print(f\"❌ Checkpoint not found at {checkpoint_path}. Train the model first.\")\n",
        "        return None\n",
        "\n",
        "    # Load checkpoint\n",
        "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
        "    class_to_idx = ckpt[\"class_to_idx\"]\n",
        "    idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
        "\n",
        "    # Build and load model\n",
        "    model = build_model(len(class_to_idx))\n",
        "    model.load_state_dict(ckpt[\"model_state\"])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Transform for single image (same as val)\n",
        "    tf = transforms.Compose([\n",
        "        transforms.Grayscale(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    img = tf(img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        pred_idx = probs.argmax(dim=1).item()\n",
        "        pred_label = idx_to_class[pred_idx]\n",
        "        confidence = probs[0, pred_idx].item()\n",
        "\n",
        "    print(f\"🖼️ Prediction: '{pred_label}' (confidence={confidence:.2f})\")\n",
        "    return pred_label, confidence\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 10) MAIN EXECUTION BLOCK\n",
        "# -----------------------------\n",
        "\n",
        "seed_everything(123)\n",
        "\n",
        "if generate:\n",
        "    generate_synthetic_dataset(fonts_dir, synth_out_dir, images_per_char=images_per_char)\n",
        "    # also split any handwritten you already have\n",
        "    if raw_hand_dir.exists():\n",
        "        split_handwritten(raw_hand_dir, hand_out_dir)\n",
        "    merge_datasets(synth_out_dir, hand_out_dir, final_out_dir)\n",
        "\n",
        "if train:\n",
        "    if not (final_out_dir / \"train\").exists():\n",
        "        print(\"Merged training data not found. Please set `generate = True` first or manually prepare data/final/...\")\n",
        "    else:\n",
        "        train(final_out_dir, epochs=epochs, batch_size=batch_size, lr=lr, patience=patience, checkpoint_path=checkpoint_path)\n",
        "\n",
        "if eval:\n",
        "    evaluate(final_out_dir, checkpoint_path=checkpoint_path)\n",
        "\n",
        "if predict is not None:  # predict is string path now\n",
        "    predict_image(Path(predict), checkpoint_path=checkpoint_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2954055144.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageOps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageFilter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2667\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2668\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m \u001b[0;31m# Enable CUDA Sanitizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   8014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8016\u001b[0;31m \u001b[0mactivate_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_meta_registrations.py\u001b[0m in \u001b[0;36mactivate_meta\u001b[0;34m()\u001b[0m\n\u001b[1;32m   8011\u001b[0m                 )\n\u001b[1;32m   8012\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8013\u001b[0;31m                 \u001b[0m_meta_lib_dont_use_me_use_register_meta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_overload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/library.py\u001b[0m in \u001b[0;36mimpl\u001b[0;34m(self, op_name, fn, dispatch_key, with_keyset, allow_override)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         self.m.impl(\n\u001b[0m\u001b[1;32m    382\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mdispatch_key\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdispatch_key\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"CompositeImplicitAutograd\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_lock_unlock_module\u001b[0;34m(name)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c3a3572",
        "outputId": "611b60b3-3a8b-4faf-cafb-9c53dc890b0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Install a compatible version of PyTorch with CUDA support\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Boo_pAMRE9Z",
        "outputId": "4b5fda6a-71c9-435e-aaf0-4a7a8353d4d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directories: assets, assets/fonts, data/raw_handwritten, data/synth, data/handwritten, data/final, checkpoints\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Define the paths for the directories\n",
        "assets_dir = Path(\"assets\")\n",
        "fonts_dir = assets_dir / \"fonts\"\n",
        "raw_handwritten_dir = Path(\"data/raw_handwritten\")\n",
        "synth_out_dir = Path(\"data/synth\")\n",
        "handwritten_out_dir = Path(\"data/handwritten\")\n",
        "final_out_dir = Path(\"data/final\")\n",
        "checkpoints_dir = Path(\"checkpoints\")\n",
        "\n",
        "\n",
        "# Create the directories if they don't exist\n",
        "assets_dir.mkdir(exist_ok=True)\n",
        "fonts_dir.mkdir(exist_ok=True)\n",
        "raw_handwritten_dir.mkdir(parents=True, exist_ok=True)\n",
        "synth_out_dir.mkdir(parents=True, exist_ok=True)\n",
        "handwritten_out_dir.mkdir(parents=True, exist_ok=True)\n",
        "final_out_dir.mkdir(parents=True, exist_ok=True)\n",
        "checkpoints_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Created directories: {assets_dir}, {fonts_dir}, {raw_handwritten_dir}, {synth_out_dir}, {handwritten_out_dir}, {final_out_dir}, {checkpoints_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGNznq0WRLR5",
        "outputId": "a8e1485f-db72-4145-ace1-0d962d33ece4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-22 08:20:05--  https://releases.pagure.org/lohit/lohit-gurmukhi-ttf-2.91.2.tar.gz\n",
            "Resolving releases.pagure.org (releases.pagure.org)... 8.43.85.76, 2620:52:3:1:dead:beef:cafe:fed8\n",
            "Connecting to releases.pagure.org (releases.pagure.org)|8.43.85.76|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18397 (18K) [application/x-gzip]\n",
            "Saving to: ‘lohit-gurmukhi-ttf-2.91.2.tar.gz’\n",
            "\n",
            "lohit-gurmukhi-ttf- 100%[===================>]  17.97K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-08-22 08:20:06 (189 KB/s) - ‘lohit-gurmukhi-ttf-2.91.2.tar.gz’ saved [18397/18397]\n",
            "\n",
            "lohit-gurmukhi-ttf-2.91.2/\n",
            "lohit-gurmukhi-ttf-2.91.2/io.pagure.lohit.gurmukhi.font.metainfo.xml\n",
            "lohit-gurmukhi-ttf-2.91.2/COPYRIGHT\n",
            "lohit-gurmukhi-ttf-2.91.2/Lohit-Gurmukhi.ttf\n",
            "lohit-gurmukhi-ttf-2.91.2/ChangeLog\n",
            "lohit-gurmukhi-ttf-2.91.2/AUTHORS\n",
            "lohit-gurmukhi-ttf-2.91.2/66-lohit-gurmukhi.conf\n",
            "lohit-gurmukhi-ttf-2.91.2/README\n",
            "lohit-gurmukhi-ttf-2.91.2/OFL.txt\n"
          ]
        }
      ],
      "source": [
        "!wget https://releases.pagure.org/lohit/lohit-gurmukhi-ttf-2.91.2.tar.gz\n",
        "!tar -xvzf lohit-gurmukhi-ttf-2.91.2.tar.gz -C assets/fonts/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNy__bx1ROO7"
      },
      "outputs": [],
      "source": [
        "!mv assets/fonts/lohit-gurmukhi-ttf-2.91.2/Lohit-Gurmukhi.ttf assets/fonts/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6hJbM2wQ81E",
        "outputId": "945aa5df-8ac5-4cd5-e936-1b5d02a755d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (1.16.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2025.6.11)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (0.4)\n"
          ]
        }
      ],
      "source": [
        "%pip install scikit-image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5b2f19e"
      },
      "source": [
        "# Task\n",
        "Update the code to allow image uploads from Google Drive for prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a1a95a4"
      },
      "source": [
        "## Update imports\n",
        "\n",
        "### Subtask:\n",
        "Add necessary imports for Google Drive integration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81d27870"
      },
      "source": [
        "**Reasoning**:\n",
        "Add the necessary import statements for Google Drive integration to the main script.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c235b79"
      },
      "source": [
        "**Reasoning**:\n",
        "The module `google.colab.vcheck` is not found. Remove the import statement `from google.colab.vcheck import SkipTest`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75a14f42"
      },
      "source": [
        "## Authenticate google drive\n",
        "\n",
        "### Subtask:\n",
        "Add a cell to authenticate and mount Google Drive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50dc8b37"
      },
      "source": [
        "**Reasoning**:\n",
        "Add a cell to authenticate and mount Google Drive as requested by the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b7d3260"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to mount Google Drive failed due to a credential propagation error. This often indicates a temporary issue with the Colab environment's ability to authenticate. Retrying the mount command is the standard procedure to resolve this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ab72d6a"
      },
      "source": [
        "## Modify prediction function\n",
        "\n",
        "### Subtask:\n",
        "Update the `predict_image` function to handle Google Drive file IDs and download the image.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "465a4d15"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the predict_image function to handle Google Drive file paths and update the prediction message.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ba9e46a"
      },
      "source": [
        "**Reasoning**:\n",
        "The KeyError indicates an issue with mapping the predicted index to a character label. The `idx_to_class` dictionary seems to be empty or incorrectly populated. This might be due to how `class_to_idx` is loaded and then inverted. I will regenerate the `predict_image` function, ensuring the `idx_to_class` dictionary is correctly created.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2759283"
      },
      "source": [
        "## Update execution block\n",
        "\n",
        "### Subtask:\n",
        "Modify the main execution block to accept a Google Drive file ID for prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10d14871"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the main execution block to pass the predict variable to the predict_image function when prediction is active.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fbae2f6"
      },
      "source": [
        "**Reasoning**:\n",
        "Correct the NameError by changing `final_out_out_dir` to `final_out_dir` in the train function call within the main execution block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4478334"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The necessary import `from google.colab import drive` was successfully added to the script for Google Drive integration.\n",
        "*   An attempt to import `google.colab.vcheck` failed with a `ModuleNotFoundError`, indicating it was not a valid or accessible module.\n",
        "*   Google Drive authentication and mounting were successfully performed at `/content/drive` after an initial transient error.\n",
        "*   The `predict_image` function was updated to handle image paths that could potentially reside in the mounted Google Drive (`/content/drive/My Drive/...`).\n",
        "*   A `KeyError` occurred during prediction in an intermediate step, which was resolved by correctly populating the `idx_to_class` mapping.\n",
        "*   The main execution block was modified to correctly pass the specified prediction image path (or potential Google Drive ID/path) to the updated `predict_image` function.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current implementation relies on Google Drive being mounted to `/content/drive`. For robustness, consider adding a check within `predict_image` to confirm if `/content/drive` is mounted before attempting to access paths within it, or provide clearer instructions to the user about mounting.\n",
        "*   While the code now handles Google Drive *paths*, the original request mentioned handling Google Drive *file IDs*. If predicting directly from file IDs without mounting is a requirement, the `predict_image` function would need further modification to use the Google Drive API or `gdown` to download the file locally based on the ID before processing.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPxmhD+DU8CJsX+bWGTAy0y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}